{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ea6c5f3",
   "metadata": {},
   "source": [
    "# Emotion Recognition Classification Task \n",
    "\n",
    "## CS985 Deep Learning - GROUP S\n",
    "\n",
    "#### Aishwarya Kurhe - 202382549                                                                 \n",
    "#### Ayesha Gaikwad - 202363500\n",
    "#### Krish Dasani - 202388555                                                                      \n",
    "#### Santosh Bangera - 202366764"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62d8750",
   "metadata": {},
   "source": [
    "## 1. Overview\n",
    "\n",
    "For the given classification task, machine learning models were developed, demonstrating the application of deep learning in emotion recognition from images. A RandomForest classifier was established as the baseline model, followed by exploration of deep neural network (DNN) architectures, integrating techniques such as data augmentation and early stopping to boost performance and address overfitting concerns. The best-performing DNN model was selected based on validation accuracy, proving its superiority over the baseline. Overall, the key learnings emphasize on the importance of thorough data preprocessing, selecting appropriate model architecture and evaluation strategies for developing effective machine learning models. Based on the modelling, possible recommendations include additional hyperparameter tuning and investigating more intricate model architectures to further optimize performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5325e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae10afa8",
   "metadata": {},
   "source": [
    "## 2. Methodology\n",
    "\n",
    "### 2.1 Exploring the data\n",
    "\n",
    "To approach modelling for this task, the process initiated with an extensive analysis of the loaded dataset that provided a better understanding of its characteristics, such as class distribution, potential imbalances, and the nature of the images themselves. The dataset show grayscale images depicting facial expressions across seven emotion classes. While class distribution appeared balanced, some classes may have fewer samples. Images exhibit diverse expressions, lighting conditions, and orientations, indicating the complexity of emotion recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4f7dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_df = pd.read_csv('my_emotion_train.csv')\n",
    "test_df = pd.read_csv('my_emotion_test.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32859bab",
   "metadata": {},
   "source": [
    "### 2.2 Data Preprocessing\n",
    "\n",
    "Subsequently, the data underwent preprocessing, with pixel values originally provided as strings converted into NumPy arrays and normalised to a range between 0 to 1 to facilitate convergence during training. \n",
    "In regards to the methodology, one of the most important aspects was the separation of the dataset into three distinct sets: training, validation, and testing. This ensured that models were trained on a portion of the data, validated on another portion to monitor performance and hyperparameters, and ultimately evaluated on unseen test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcd5315e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess function to convert pixels from string to numpy array\n",
    "def preprocess_pixels(pixel_str):\n",
    "    return np.array([int(pixel) for pixel in pixel_str.split()]).reshape(48, 48, 1)\n",
    "\n",
    "# Apply preprocessing\n",
    "X_train = np.array([preprocess_pixels(x) for x in train_df['pixels']])\n",
    "y_train = to_categorical(train_df['emotion'])\n",
    "X_test = np.array([preprocess_pixels(x) for x in test_df['pixels']])\n",
    "\n",
    "# Normalize pixel values\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477a781",
   "metadata": {},
   "source": [
    "### 2.3 Data Augmentation\n",
    "\n",
    "Given the limited size of the dataset, the utilization of data augmentation techniques was considered crucial. Techniques such as rotation, flipping, and zooming adjustment were employed to enhance model generalization and improve performance by introducing variability in the training data. By diversifying the dataset, these augmentation techniques aimed to give the model access to a bigger and more representative sample of data for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae95489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data augmentation\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "datagen.fit(X_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69edd1c2",
   "metadata": {},
   "source": [
    "## 3. Models\n",
    "\n",
    "Model selection for the task involved exploring both traditional machine learning approaches and deep neural networks (DNNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520a940d",
   "metadata": {},
   "source": [
    "### 3.1 Standard ML Baseline Model\n",
    "\n",
    "The RandomForest classifier was selected as the Standard ML Baseline model stems from its simplicity, effectiveness, and suitability for tabular data like the one in this dataset. RandomForest is an ensemble learning method based on decision trees, where multiple decision trees are trained on random subsets of the data and then aggregated to make predictions.\n",
    "In this implementation, the RandomForestClassifier was instantiated with 100 estimators (decision trees) and a random state of 42 for reproducibility. The 'n_estimators' parameter controlled the number of decision trees in the forest, and a higher number generally led to better performance, albeit with increased computational cost.\n",
    "\n",
    "Before fitting the RandomForest model, the input data was preprocessed to meet the classifier's specifications. Because RandomForest works with flattened input arrays rather than multidimensional arrays, the input data (X_train) was reshaped using'reshape()' to convert each image into a one-dimensional array while keeping the number of samples. Likewise, the target labels (y_train) were transformed with 'argmax(axis=1)' to return the one-hot encoded labels to their original integer format, which is suitable for RandomForest classification.\n",
    "\n",
    "The RandomForest model's performance was evaluated using cross-validation. Cross-validation was a reliable technique for estimating a model's performance by dividing the dataset into multiple subsets (folds), training the model on one subset, and testing it on the remaining data. The 'cross_val_score' function from scikit-learn was used in conjunction with a 5-fold cross-validation strategy ('cv=5'). This meant that the dataset was divided into five equal-sized folds, and the model was trained and tested five times, with each fold serving as a validation set once.\n",
    "\n",
    "The mean accuracy and standard deviation of the cross-validation scores were computed and stored in the 'model_performance' list, along with the model name ('RandomForest'). These metrics provided information about the model's average performance and variability across various folds. Thus, the RandomForest classifier was utilised as a baseline for comparing the performance of more complex models, such as deep neural networks, in subsequent analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98718e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for model comparisons\n",
    "model_performance = []\n",
    "\n",
    "# Standard ML Baseline with RandomForest\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "# Flatten X for RandomForest\n",
    "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "y_train_flat = y_train.argmax(axis=1)\n",
    "# Cross-validation for RandomForest model\n",
    "rf_cv_scores = cross_val_score(rf_model, X_train_flat, y_train_flat, cv=5)\n",
    "model_performance.append({\n",
    "    'Model': 'RandomForest',\n",
    "    'Mean Accuracy': rf_cv_scores.mean(),\n",
    "    'Std Deviation': rf_cv_scores.std()\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c832587d",
   "metadata": {},
   "source": [
    "### 3.2 Deep NN models\n",
    "\n",
    "In the exploration of deep neural network (DNN) models, a sequential model architecture was adopted, featuring convolutional layers followed by max-pooling and batch normalisation. The chosen architecture, a Sequential model, was designed specifically for image classification tasks, beginning with convolutional layers that extracted features from input images. Each convolutional layer was combined with max-pooling to downsample feature maps and batch normalisation to achieve better stability and speed up training.\n",
    "\n",
    "The model architecture commenced with a Conv2D layer employing 32 filters and a 3x3 kernel size, followed by a ReLU activation function. Subsequent layers deepened the network's capacity by increasing the number of filters in the convolutional layers (64 and 128), further enhancing its ability to discern intricate patterns within the images. Batch normalization after each convolutional layer aided in normalizing activations, promoting faster convergence and mitigating issues related to internal covariate shift.\n",
    "\n",
    "Following the convolutional layers, the feature maps were flattened into a one-dimensional array to transition into fully connected layers. Here, two Dense layers were employed, each containing 256 neurons activated by ReLU, with a dropout layer to prevent overfitting. The final Dense layer had 7 neurons, representing the 7 emotion classes in the dataset, and utilized a softmax activation function to output class probabilities. During training, the model underwent optimization using the Adam optimizer with a learning rate set to 0.001 and minimized categorical cross-entropy loss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69943d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep NN models exploration\n",
    "val_accuracies = []\n",
    "\n",
    "def create_deep_nn_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        BatchNormalization(),\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        BatchNormalization(),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(7, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf78d8a",
   "metadata": {},
   "source": [
    "To reduce overfitting, two key strategies were applied during training. Early stopping was used to monitor validation accuracy, halting training if performance did not improve after a set number of epochs. This impeded the model from overmemorizing the training data, allowing it to retain generalisation capabilities. Moreover, a learning rate scheduler dynamically adjusted the learning rate during training, starting at a high value for rapid convergence and gradually decreasing to fine-tune model parameters as training progressed.\n",
    "\n",
    "Five different model configurations were evaluated for optimal performance. This iterative process entailed adjusting various architectural components and training parameters, including convolutional layer configurations, dropout rates, and batch sizes. Each configuration was trained and evaluated on the validation set, with the model that achieved the highest validation accuracy being chosen for further analysis. By systematically refining the model architecture and training regimen, the goal was to develop a robust DNN model capable of accurately recognizing emotions from facial expressions across diverse settings and conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6fb601fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOME\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HOME\\anaconda3\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 193ms/step - accuracy: 0.2620 - loss: 2.2697 - val_accuracy: 0.2895 - val_loss: 1.9834 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 167ms/step - accuracy: 0.3669 - loss: 1.6367 - val_accuracy: 0.4471 - val_loss: 1.4293 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 161ms/step - accuracy: 0.4164 - loss: 1.5184 - val_accuracy: 0.4016 - val_loss: 1.5329 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 158ms/step - accuracy: 0.4435 - loss: 1.4502 - val_accuracy: 0.4950 - val_loss: 1.3264 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 174ms/step - accuracy: 0.4720 - loss: 1.3785 - val_accuracy: 0.5059 - val_loss: 1.2928 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 175ms/step - accuracy: 0.4917 - loss: 1.3444 - val_accuracy: 0.5002 - val_loss: 1.2945 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 181ms/step - accuracy: 0.5093 - loss: 1.3012 - val_accuracy: 0.5271 - val_loss: 1.2318 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 167ms/step - accuracy: 0.5171 - loss: 1.2755 - val_accuracy: 0.5183 - val_loss: 1.2479 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 159ms/step - accuracy: 0.5262 - loss: 1.2527 - val_accuracy: 0.5216 - val_loss: 1.2632 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 160ms/step - accuracy: 0.5356 - loss: 1.2225 - val_accuracy: 0.4876 - val_loss: 1.3164 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 179ms/step - accuracy: 0.5440 - loss: 1.2041 - val_accuracy: 0.5533 - val_loss: 1.2011 - learning_rate: 9.0484e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 156ms/step - accuracy: 0.5613 - loss: 1.1722 - val_accuracy: 0.5591 - val_loss: 1.1715 - learning_rate: 8.1873e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 116ms/step - accuracy: 0.5593 - loss: 1.1590 - val_accuracy: 0.5628 - val_loss: 1.1777 - learning_rate: 7.4082e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 108ms/step - accuracy: 0.5748 - loss: 1.1368 - val_accuracy: 0.5888 - val_loss: 1.0920 - learning_rate: 6.7032e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 101ms/step - accuracy: 0.5874 - loss: 1.1205 - val_accuracy: 0.5667 - val_loss: 1.1412 - learning_rate: 6.0653e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 102ms/step - accuracy: 0.5896 - loss: 1.0931 - val_accuracy: 0.5948 - val_loss: 1.0885 - learning_rate: 5.4881e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 114ms/step - accuracy: 0.5875 - loss: 1.0838 - val_accuracy: 0.5997 - val_loss: 1.0693 - learning_rate: 4.9659e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 140ms/step - accuracy: 0.5887 - loss: 1.0853 - val_accuracy: 0.5821 - val_loss: 1.1101 - learning_rate: 4.4933e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 120ms/step - accuracy: 0.6011 - loss: 1.0608 - val_accuracy: 0.5752 - val_loss: 1.1416 - learning_rate: 4.0657e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 117ms/step - accuracy: 0.6126 - loss: 1.0350 - val_accuracy: 0.5976 - val_loss: 1.0785 - learning_rate: 3.6788e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 116ms/step - accuracy: 0.6139 - loss: 1.0227 - val_accuracy: 0.6129 - val_loss: 1.0424 - learning_rate: 3.3287e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 115ms/step - accuracy: 0.6127 - loss: 1.0241 - val_accuracy: 0.6014 - val_loss: 1.0615 - learning_rate: 3.0119e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 103ms/step - accuracy: 0.6237 - loss: 1.0037 - val_accuracy: 0.6109 - val_loss: 1.0503 - learning_rate: 2.7253e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 103ms/step - accuracy: 0.6255 - loss: 1.0000 - val_accuracy: 0.6198 - val_loss: 1.0315 - learning_rate: 2.4660e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 104ms/step - accuracy: 0.6275 - loss: 0.9932 - val_accuracy: 0.6097 - val_loss: 1.0714 - learning_rate: 2.2313e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 122ms/step - accuracy: 0.6313 - loss: 0.9791 - val_accuracy: 0.6122 - val_loss: 1.0480 - learning_rate: 2.0190e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 113ms/step - accuracy: 0.6398 - loss: 0.9724 - val_accuracy: 0.6109 - val_loss: 1.0578 - learning_rate: 1.8268e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 117ms/step - accuracy: 0.6381 - loss: 0.9626 - val_accuracy: 0.6088 - val_loss: 1.0720 - learning_rate: 1.6530e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 123ms/step - accuracy: 0.6338 - loss: 0.9677 - val_accuracy: 0.6091 - val_loss: 1.0827 - learning_rate: 1.4957e-04\n",
      "Epoch 1/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 133ms/step - accuracy: 0.2506 - loss: 2.2792 - val_accuracy: 0.3022 - val_loss: 1.7123 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 124ms/step - accuracy: 0.3668 - loss: 1.6394 - val_accuracy: 0.4219 - val_loss: 1.5295 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 125ms/step - accuracy: 0.4074 - loss: 1.5304 - val_accuracy: 0.4779 - val_loss: 1.3952 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 123ms/step - accuracy: 0.4451 - loss: 1.4496 - val_accuracy: 0.4036 - val_loss: 1.5411 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 117ms/step - accuracy: 0.4719 - loss: 1.3842 - val_accuracy: 0.4709 - val_loss: 1.3745 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 116ms/step - accuracy: 0.4846 - loss: 1.3459 - val_accuracy: 0.5312 - val_loss: 1.2604 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 100ms/step - accuracy: 0.5010 - loss: 1.3030 - val_accuracy: 0.4812 - val_loss: 1.3415 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 100ms/step - accuracy: 0.5206 - loss: 1.2711 - val_accuracy: 0.4993 - val_loss: 1.3067 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 99ms/step - accuracy: 0.5227 - loss: 1.2539 - val_accuracy: 0.4819 - val_loss: 1.4352 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.5301 - loss: 1.2288 - val_accuracy: 0.5319 - val_loss: 1.2507 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 124ms/step - accuracy: 0.5443 - loss: 1.2048 - val_accuracy: 0.5414 - val_loss: 1.2103 - learning_rate: 9.0484e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 119ms/step - accuracy: 0.5479 - loss: 1.1934 - val_accuracy: 0.5653 - val_loss: 1.1436 - learning_rate: 8.1873e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.5654 - loss: 1.1606 - val_accuracy: 0.5362 - val_loss: 1.2479 - learning_rate: 7.4082e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 117ms/step - accuracy: 0.5682 - loss: 1.1357 - val_accuracy: 0.5693 - val_loss: 1.1578 - learning_rate: 6.7032e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.5709 - loss: 1.1261 - val_accuracy: 0.5750 - val_loss: 1.1342 - learning_rate: 6.0653e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 110ms/step - accuracy: 0.5823 - loss: 1.1034 - val_accuracy: 0.5838 - val_loss: 1.1356 - learning_rate: 5.4881e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.5886 - loss: 1.0905 - val_accuracy: 0.5743 - val_loss: 1.1416 - learning_rate: 4.9659e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 105ms/step - accuracy: 0.6013 - loss: 1.0583 - val_accuracy: 0.6040 - val_loss: 1.0766 - learning_rate: 4.4933e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.6027 - loss: 1.0518 - val_accuracy: 0.5867 - val_loss: 1.1186 - learning_rate: 4.0657e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.6002 - loss: 1.0467 - val_accuracy: 0.5988 - val_loss: 1.0859 - learning_rate: 3.6788e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.6038 - loss: 1.0433 - val_accuracy: 0.6186 - val_loss: 1.0552 - learning_rate: 3.3287e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.6084 - loss: 1.0343 - val_accuracy: 0.5883 - val_loss: 1.1302 - learning_rate: 3.0119e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 108ms/step - accuracy: 0.6133 - loss: 1.0254 - val_accuracy: 0.6100 - val_loss: 1.0653 - learning_rate: 2.7253e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.6180 - loss: 1.0127 - val_accuracy: 0.6166 - val_loss: 1.0629 - learning_rate: 2.4660e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.6226 - loss: 0.9941 - val_accuracy: 0.6162 - val_loss: 1.0607 - learning_rate: 2.2313e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.6227 - loss: 1.0002 - val_accuracy: 0.6140 - val_loss: 1.0655 - learning_rate: 2.0190e-04\n",
      "Epoch 1/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 109ms/step - accuracy: 0.2541 - loss: 2.2825 - val_accuracy: 0.2653 - val_loss: 1.8903 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.3630 - loss: 1.6452 - val_accuracy: 0.4300 - val_loss: 1.4734 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.4032 - loss: 1.5308 - val_accuracy: 0.4753 - val_loss: 1.3574 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.4397 - loss: 1.4550 - val_accuracy: 0.4741 - val_loss: 1.3369 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 108ms/step - accuracy: 0.4678 - loss: 1.3888 - val_accuracy: 0.5150 - val_loss: 1.2667 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 108ms/step - accuracy: 0.4826 - loss: 1.3418 - val_accuracy: 0.5272 - val_loss: 1.2500 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.5026 - loss: 1.3052 - val_accuracy: 0.5028 - val_loss: 1.3165 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 108ms/step - accuracy: 0.5163 - loss: 1.2802 - val_accuracy: 0.4814 - val_loss: 1.3944 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.5275 - loss: 1.2466 - val_accuracy: 0.5484 - val_loss: 1.1858 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 106ms/step - accuracy: 0.5380 - loss: 1.2148 - val_accuracy: 0.5609 - val_loss: 1.1893 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 108ms/step - accuracy: 0.5452 - loss: 1.2082 - val_accuracy: 0.5503 - val_loss: 1.1793 - learning_rate: 9.0484e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.5541 - loss: 1.1766 - val_accuracy: 0.5159 - val_loss: 1.2718 - learning_rate: 8.1873e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.5659 - loss: 1.1522 - val_accuracy: 0.5874 - val_loss: 1.1053 - learning_rate: 7.4082e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 106ms/step - accuracy: 0.5719 - loss: 1.1199 - val_accuracy: 0.5191 - val_loss: 1.2611 - learning_rate: 6.7032e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.5752 - loss: 1.1234 - val_accuracy: 0.5805 - val_loss: 1.1368 - learning_rate: 6.0653e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 115ms/step - accuracy: 0.5923 - loss: 1.0863 - val_accuracy: 0.5860 - val_loss: 1.1167 - learning_rate: 5.4881e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 114ms/step - accuracy: 0.5891 - loss: 1.0907 - val_accuracy: 0.6069 - val_loss: 1.0668 - learning_rate: 4.9659e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.6016 - loss: 1.0579 - val_accuracy: 0.5700 - val_loss: 1.1395 - learning_rate: 4.4933e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 108ms/step - accuracy: 0.5940 - loss: 1.0730 - val_accuracy: 0.5883 - val_loss: 1.1128 - learning_rate: 4.0657e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.6075 - loss: 1.0465 - val_accuracy: 0.5876 - val_loss: 1.1050 - learning_rate: 3.6788e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.6138 - loss: 1.0290 - val_accuracy: 0.6060 - val_loss: 1.0740 - learning_rate: 3.3287e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.6129 - loss: 1.0315 - val_accuracy: 0.5884 - val_loss: 1.1208 - learning_rate: 3.0119e-04\n",
      "Epoch 1/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 111ms/step - accuracy: 0.2623 - loss: 2.2511 - val_accuracy: 0.2869 - val_loss: 1.7186 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.3814 - loss: 1.6166 - val_accuracy: 0.4279 - val_loss: 1.4543 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.4302 - loss: 1.4783 - val_accuracy: 0.4724 - val_loss: 1.3838 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.4524 - loss: 1.4111 - val_accuracy: 0.4866 - val_loss: 1.3110 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.4861 - loss: 1.3618 - val_accuracy: 0.4757 - val_loss: 1.3465 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.4874 - loss: 1.3299 - val_accuracy: 0.5214 - val_loss: 1.2468 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.5080 - loss: 1.2963 - val_accuracy: 0.4736 - val_loss: 1.3388 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.5144 - loss: 1.2702 - val_accuracy: 0.5326 - val_loss: 1.2433 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 113ms/step - accuracy: 0.5255 - loss: 1.2457 - val_accuracy: 0.5226 - val_loss: 1.2632 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 105ms/step - accuracy: 0.5307 - loss: 1.2297 - val_accuracy: 0.5453 - val_loss: 1.2215 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.5411 - loss: 1.2069 - val_accuracy: 0.5545 - val_loss: 1.1700 - learning_rate: 9.0484e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.5639 - loss: 1.1581 - val_accuracy: 0.5588 - val_loss: 1.1692 - learning_rate: 8.1873e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.5642 - loss: 1.1606 - val_accuracy: 0.5784 - val_loss: 1.1505 - learning_rate: 7.4082e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 105ms/step - accuracy: 0.5736 - loss: 1.1275 - val_accuracy: 0.5853 - val_loss: 1.0908 - learning_rate: 6.7032e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 108ms/step - accuracy: 0.5706 - loss: 1.1242 - val_accuracy: 0.5874 - val_loss: 1.1042 - learning_rate: 6.0653e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.5855 - loss: 1.0934 - val_accuracy: 0.5914 - val_loss: 1.0889 - learning_rate: 5.4881e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.5916 - loss: 1.0804 - val_accuracy: 0.5702 - val_loss: 1.2029 - learning_rate: 4.9659e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.5953 - loss: 1.0670 - val_accuracy: 0.5943 - val_loss: 1.0932 - learning_rate: 4.4933e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 105ms/step - accuracy: 0.6077 - loss: 1.0502 - val_accuracy: 0.5926 - val_loss: 1.1073 - learning_rate: 4.0657e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.6167 - loss: 1.0361 - val_accuracy: 0.5966 - val_loss: 1.1035 - learning_rate: 3.6788e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.6113 - loss: 1.0344 - val_accuracy: 0.5821 - val_loss: 1.1414 - learning_rate: 3.3287e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.6133 - loss: 1.0217 - val_accuracy: 0.6059 - val_loss: 1.0496 - learning_rate: 3.0119e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 106ms/step - accuracy: 0.6192 - loss: 1.0142 - val_accuracy: 0.6128 - val_loss: 1.0355 - learning_rate: 2.7253e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 106ms/step - accuracy: 0.6192 - loss: 1.0056 - val_accuracy: 0.6034 - val_loss: 1.0621 - learning_rate: 2.4660e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.6223 - loss: 1.0104 - val_accuracy: 0.6066 - val_loss: 1.0623 - learning_rate: 2.2313e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.6278 - loss: 0.9840 - val_accuracy: 0.6117 - val_loss: 1.0472 - learning_rate: 2.0190e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.6371 - loss: 0.9755 - val_accuracy: 0.6119 - val_loss: 1.0412 - learning_rate: 1.8268e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 105ms/step - accuracy: 0.6413 - loss: 0.9689 - val_accuracy: 0.6198 - val_loss: 1.0417 - learning_rate: 1.6530e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.6332 - loss: 0.9766 - val_accuracy: 0.6229 - val_loss: 1.0323 - learning_rate: 1.4957e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.6328 - loss: 0.9662 - val_accuracy: 0.6222 - val_loss: 1.0280 - learning_rate: 1.3534e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.6421 - loss: 0.9662 - val_accuracy: 0.6281 - val_loss: 1.0267 - learning_rate: 1.2246e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 106ms/step - accuracy: 0.6456 - loss: 0.9454 - val_accuracy: 0.6259 - val_loss: 1.0120 - learning_rate: 1.1080e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 107ms/step - accuracy: 0.6368 - loss: 0.9558 - val_accuracy: 0.6203 - val_loss: 1.0279 - learning_rate: 1.0026e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.6400 - loss: 0.9543 - val_accuracy: 0.6248 - val_loss: 1.0164 - learning_rate: 9.0718e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.6430 - loss: 0.9611 - val_accuracy: 0.6295 - val_loss: 1.0138 - learning_rate: 8.2085e-05\n",
      "Epoch 36/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.6443 - loss: 0.9452 - val_accuracy: 0.6291 - val_loss: 1.0155 - learning_rate: 7.4274e-05\n",
      "Epoch 37/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 105ms/step - accuracy: 0.6470 - loss: 0.9382 - val_accuracy: 0.6224 - val_loss: 1.0254 - learning_rate: 6.7206e-05\n",
      "Epoch 38/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.6495 - loss: 0.9368 - val_accuracy: 0.6298 - val_loss: 1.0155 - learning_rate: 6.0810e-05\n",
      "Epoch 39/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.6507 - loss: 0.9322 - val_accuracy: 0.6264 - val_loss: 1.0227 - learning_rate: 5.5023e-05\n",
      "Epoch 40/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.6542 - loss: 0.9248 - val_accuracy: 0.6253 - val_loss: 1.0181 - learning_rate: 4.9787e-05\n",
      "Epoch 41/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 113ms/step - accuracy: 0.6490 - loss: 0.9376 - val_accuracy: 0.6310 - val_loss: 1.0140 - learning_rate: 4.5049e-05\n",
      "Epoch 42/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.6496 - loss: 0.9197 - val_accuracy: 0.6314 - val_loss: 1.0133 - learning_rate: 4.0762e-05\n",
      "Epoch 43/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.6494 - loss: 0.9310 - val_accuracy: 0.6300 - val_loss: 1.0094 - learning_rate: 3.6883e-05\n",
      "Epoch 44/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 111ms/step - accuracy: 0.6528 - loss: 0.9284 - val_accuracy: 0.6293 - val_loss: 1.0066 - learning_rate: 3.3373e-05\n",
      "Epoch 45/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.6465 - loss: 0.9383 - val_accuracy: 0.6331 - val_loss: 1.0066 - learning_rate: 3.0197e-05\n",
      "Epoch 46/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.6490 - loss: 0.9325 - val_accuracy: 0.6347 - val_loss: 1.0057 - learning_rate: 2.7324e-05\n",
      "Epoch 47/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.6522 - loss: 0.9280 - val_accuracy: 0.6322 - val_loss: 1.0046 - learning_rate: 2.4724e-05\n",
      "Epoch 48/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 113ms/step - accuracy: 0.6515 - loss: 0.9190 - val_accuracy: 0.6367 - val_loss: 1.0050 - learning_rate: 2.2371e-05\n",
      "Epoch 49/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 110ms/step - accuracy: 0.6543 - loss: 0.9177 - val_accuracy: 0.6340 - val_loss: 1.0046 - learning_rate: 2.0242e-05\n",
      "Epoch 50/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 107ms/step - accuracy: 0.6541 - loss: 0.9277 - val_accuracy: 0.6345 - val_loss: 1.0041 - learning_rate: 1.8316e-05\n",
      "Epoch 1/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 116ms/step - accuracy: 0.2541 - loss: 2.2596 - val_accuracy: 0.3321 - val_loss: 1.6960 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.3636 - loss: 1.6226 - val_accuracy: 0.4543 - val_loss: 1.4148 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.4227 - loss: 1.5017 - val_accuracy: 0.4721 - val_loss: 1.3734 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 106ms/step - accuracy: 0.4530 - loss: 1.4217 - val_accuracy: 0.4736 - val_loss: 1.3434 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 109ms/step - accuracy: 0.4765 - loss: 1.3674 - val_accuracy: 0.4483 - val_loss: 1.4200 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.4931 - loss: 1.3188 - val_accuracy: 0.4840 - val_loss: 1.3932 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 113ms/step - accuracy: 0.5113 - loss: 1.2866 - val_accuracy: 0.5174 - val_loss: 1.2868 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 113ms/step - accuracy: 0.5203 - loss: 1.2616 - val_accuracy: 0.5329 - val_loss: 1.2253 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 106ms/step - accuracy: 0.5276 - loss: 1.2447 - val_accuracy: 0.5541 - val_loss: 1.1891 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.5376 - loss: 1.2258 - val_accuracy: 0.5319 - val_loss: 1.2286 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.5464 - loss: 1.2014 - val_accuracy: 0.5450 - val_loss: 1.2164 - learning_rate: 9.0484e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 114ms/step - accuracy: 0.5611 - loss: 1.1656 - val_accuracy: 0.5759 - val_loss: 1.1474 - learning_rate: 8.1873e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 106ms/step - accuracy: 0.5658 - loss: 1.1531 - val_accuracy: 0.5683 - val_loss: 1.1482 - learning_rate: 7.4082e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.5739 - loss: 1.1288 - val_accuracy: 0.5548 - val_loss: 1.1833 - learning_rate: 6.7032e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 113ms/step - accuracy: 0.5788 - loss: 1.1047 - val_accuracy: 0.5731 - val_loss: 1.1517 - learning_rate: 6.0653e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 113ms/step - accuracy: 0.5863 - loss: 1.0953 - val_accuracy: 0.5950 - val_loss: 1.0952 - learning_rate: 5.4881e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.5908 - loss: 1.0873 - val_accuracy: 0.6062 - val_loss: 1.0656 - learning_rate: 4.9659e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 108ms/step - accuracy: 0.5976 - loss: 1.0617 - val_accuracy: 0.6091 - val_loss: 1.0584 - learning_rate: 4.4933e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.5907 - loss: 1.0728 - val_accuracy: 0.5638 - val_loss: 1.1639 - learning_rate: 4.0657e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.6102 - loss: 1.0369 - val_accuracy: 0.6128 - val_loss: 1.0607 - learning_rate: 3.6788e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.6060 - loss: 1.0394 - val_accuracy: 0.5831 - val_loss: 1.1059 - learning_rate: 3.3287e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 108ms/step - accuracy: 0.6199 - loss: 1.0118 - val_accuracy: 0.6124 - val_loss: 1.0534 - learning_rate: 3.0119e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 114ms/step - accuracy: 0.6213 - loss: 1.0192 - val_accuracy: 0.6126 - val_loss: 1.0529 - learning_rate: 2.7253e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 114ms/step - accuracy: 0.6177 - loss: 1.0185 - val_accuracy: 0.6214 - val_loss: 1.0391 - learning_rate: 2.4660e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 114ms/step - accuracy: 0.6353 - loss: 0.9903 - val_accuracy: 0.6153 - val_loss: 1.0410 - learning_rate: 2.2313e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.6211 - loss: 0.9942 - val_accuracy: 0.6234 - val_loss: 1.0335 - learning_rate: 2.0190e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 111ms/step - accuracy: 0.6294 - loss: 0.9852 - val_accuracy: 0.6190 - val_loss: 1.0493 - learning_rate: 1.8268e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 113ms/step - accuracy: 0.6267 - loss: 0.9836 - val_accuracy: 0.6284 - val_loss: 1.0187 - learning_rate: 1.6530e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.6380 - loss: 0.9668 - val_accuracy: 0.6250 - val_loss: 1.0264 - learning_rate: 1.4957e-04\n",
      "Epoch 30/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.6434 - loss: 0.9477 - val_accuracy: 0.6386 - val_loss: 1.0120 - learning_rate: 1.3534e-04\n",
      "Epoch 31/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 108ms/step - accuracy: 0.6397 - loss: 0.9589 - val_accuracy: 0.6309 - val_loss: 1.0129 - learning_rate: 1.2246e-04\n",
      "Epoch 32/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 113ms/step - accuracy: 0.6439 - loss: 0.9500 - val_accuracy: 0.6347 - val_loss: 1.0173 - learning_rate: 1.1080e-04\n",
      "Epoch 33/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 113ms/step - accuracy: 0.6350 - loss: 0.9590 - val_accuracy: 0.6295 - val_loss: 1.0172 - learning_rate: 1.0026e-04\n",
      "Epoch 34/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 112ms/step - accuracy: 0.6362 - loss: 0.9594 - val_accuracy: 0.6343 - val_loss: 1.0037 - learning_rate: 9.0718e-05\n",
      "Epoch 35/50\n",
      "\u001b[1m363/363\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 108ms/step - accuracy: 0.6471 - loss: 0.9462 - val_accuracy: 0.6324 - val_loss: 1.0167 - learning_rate: 8.2085e-05\n"
     ]
    }
   ],
   "source": [
    "best_val_accuracy = 0\n",
    "best_model = None\n",
    "for i in range(5):  # Example: trying 5 different configurations\n",
    "    model = create_deep_nn_model()\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "    model_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True)\n",
    "    \n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < 10:\n",
    "            return float(lr)\n",
    "        else:\n",
    "            return float(lr * tf.math.exp(-0.1))\n",
    "\n",
    "    lr_scheduler = LearningRateScheduler(scheduler)\n",
    "    history = model.fit(datagen.flow(X_train, y_train, batch_size=64), epochs=50,\n",
    "                        validation_data=(X_val, y_val),\n",
    "                        callbacks=[early_stopping, model_checkpoint, lr_scheduler])\n",
    "    val_accuracy = history.history['val_accuracy'][-1]\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        best_model = model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306eca42",
   "metadata": {},
   "source": [
    "The performance of the best deep neural network (DNN) model chosen during the exploration process was assessed by calculating its accuracy on the validation set and appending it, along with the standard deviation, to a list named 'model_performance'. A comparison table was then created from this list, outlining the performance metrics for each model, including the best DNN model. Subsequently, the best model's weights were loaded, and predictions were made on the test dataset, with the predicted classes saved along with the corresponding IDs into a submission file. This streamlined process encapsulated the evaluation and deployment of the top-performing DNN model for emotion recognition on the provided dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "317d256a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Model  Mean Accuracy  Std Deviation\n",
      "0    RandomForest       0.438405       0.007041\n",
      "1  Best DNN Model       0.615690       0.016857\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 19ms/step\n"
     ]
    }
   ],
   "source": [
    "# Assuming best model is selected, now add its performance to comparison\n",
    "model_performance.append({\n",
    "    'Model': 'Best DNN Model',\n",
    "    'Mean Accuracy': np.mean(val_accuracies),\n",
    "    'Std Deviation': np.std(val_accuracies)\n",
    "})\n",
    "\n",
    "# Constructing the comparison table\n",
    "comparison_table = pd.DataFrame(model_performance)\n",
    "\n",
    "# Output the comparison table\n",
    "print(comparison_table)\n",
    "\n",
    "best_model.load_weights('best_model.keras')\n",
    "predictions = best_model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Prepare submission file using the 'id' column from the test dataset\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'emotion': predicted_classes})\n",
    "submission_df.to_csv('my_emotion_result_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d73a59",
   "metadata": {},
   "source": [
    "## 4. Results\n",
    "\n",
    "### 4.1 Table of results\n",
    "\n",
    "\n",
    "##### RandomForest\n",
    "\n",
    "Mean Accuracy - 0.438405\n",
    "\n",
    "Standard Deviation - 0.007041\n",
    "\n",
    "Validation Accuracy - 0.4503\n",
    "\n",
    "##### Best DNN Model\n",
    "\n",
    "Mean Accuracy - 0.615690 \n",
    "\n",
    "Standard Deviation - 0.016857\n",
    "\n",
    "Validation Accuracy - 0.6324\n",
    "\n",
    "Validation Accuracy without Augmentation - 0.582931\n",
    "\n",
    "\n",
    "For best results, two methods for emotion recognition have been studied, including a deep neural network (DNN) architecture and the RandomForest baseline model. The RandomForest model achieved a mean accuracy of 0.438405 with a standard deviation of 0.00704 through cross-validation. However, the DNN approach, with five different configurations tested, outperformed the baseline, with the best model achieving a mean accuracy of 0.615690 and a standard deviation of 0.016857.\n",
    "\n",
    "While experimenting with the DNN models, various configurations were explored, including different convolutional layer depths, pooling strategies, and regularization techniques like dropout. The introduction of data augmentation techniques further enhanced model performance by providing additional training data, since the validation accuracy was lower without it. Early stopping and learning rate scheduling were also employed to prevent overfitting and optimize training dynamics.\n",
    "\n",
    "Despite the successes, challenges were encountered during the experimentation process. Some configurations may have led to overfitting or failed to converge effectively, highlighting the delicate balance required in optimizing DNN architectures. In addition, hyperparameter tuning had the potential to improve performance even further, but extensive experimentation was limited by computational resources.\n",
    "\n",
    "In its entirety, the results affirm the potential of deep learning methodologies, particularly convolutional neural networks, in accurately discerning emotions from facial expressions, laying a foundation for continued advancement in emotion recognition research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d941c53",
   "metadata": {},
   "source": [
    "## 5. Summary\n",
    "\n",
    "As both of the baseline Random Forest classifier and deep neural network (DNN) models were tested, the best-performing DNN model was opted based on validation accuracy and utilized for predicting emotions on the test dataset. Hence, the recommended model for this task would be the deep neural network (DNN) model, due to its ability to capture intricate patterns in facial expressions through convolutional layers. The selected DNN model demonstrates superior performance compared to the baseline Random Forest classifier, as indicated by its higher mean accuracy in the validation set. \n",
    "\n",
    "Similarly, the DNN model turned out to be the best performing technique on the Kaggle submission, with a test accuracy (Kaggle score) of 0.62. Contrary to this, the Standard baseline Random Forest classifier model merely achieved a test accuracy score of 0.50.\n",
    "\n",
    "Multiple strategies can be contemplated in order to enhance performance in this task even more. Better outcomes might be obtained by experimenting with more intricate DNN architectures, modifying hyperparameters like learning rate and dropout rates, and looking into advanced techniques like transfer learning. Beyond that, expanding the variety and magnitude of the dataset and modifying the parameters for data augmentation could improve the model's capacity to generalise and adapt to a variety of facial expressions and contextual factors. Regular monitoring and updating of the model with new data can also contribute to continuous improvement in performance over time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
